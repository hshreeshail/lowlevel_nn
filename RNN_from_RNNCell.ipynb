{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing RNN using RNNCell\n",
    "## Initializing inputs and models. Insert manual_seeds carefully. We will be comparing outputs\n",
    "## to see if our implementation is correct\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "input_size = 32\n",
    "hidden_size = 16\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "cell_type = 'GRU'\n",
    "\n",
    "torch.manual_seed(3407)\n",
    "rnncell = []\n",
    "if bidirectional:\n",
    "  rnncell_rev = []\n",
    "for layer in range(num_layers):\n",
    "  ip_arg = input_size if layer == 0 else 2*hidden_size if bidirectional else hidden_size\n",
    "  if cell_type == 'Vanilla_RNN':\n",
    "    rnncell.append(nn.RNNCell(input_size=ip_arg, hidden_size=hidden_size))\n",
    "  elif cell_type == 'GRU':\n",
    "    rnncell.append(nn.GRUCell(input_size=ip_arg, hidden_size=hidden_size))\n",
    "  \n",
    "  if bidirectional:\n",
    "    if cell_type == 'Vanilla_RNN':\n",
    "      rnncell_rev.append(nn.RNNCell(input_size=ip_arg, hidden_size=hidden_size))\n",
    "    elif cell_type == 'GRU':\n",
    "      rnncell_rev.append(nn.GRUCell(input_size=ip_arg, hidden_size=hidden_size))\n",
    "  \n",
    "\n",
    "\n",
    "torch.manual_seed(3407)\n",
    "if cell_type == 'Vanilla_RNN':\n",
    "  rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
    "elif cell_type == 'GRU':\n",
    "  rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
    "\n",
    "## total rnn params\n",
    "# rnn_total_params = 0\n",
    "# for param in rnn.parameters():\n",
    "#   rnn_total_params += param.numel()\n",
    "\n",
    "## total rnncell params\n",
    "# rnncell_total_params = 0\n",
    "# for cell in (rnncell + rnncell_rev):\n",
    "#   for param in cell.parameters():\n",
    "#     rnncell_total_params += param.numel()\n",
    "\n",
    "\n",
    "## Comparing initialized model weights to check if we have same initialization\n",
    "# param_list = ['weight_ih', 'weight_hh']\n",
    "# for param in param_list:\n",
    "#   w1 = getattr(rnncell[0], param)\n",
    "#   w2 = getattr(rnncell_rev[0], param)\n",
    "#   w3 = getattr(rnncell[1], param)\n",
    "#   w4 = getattr(rnncell_rev[1], param)\n",
    "\n",
    "#   w5 = getattr(rnn, f'{param}_l0')\n",
    "#   w6 = getattr(rnn, f'{param}_l0_reverse')\n",
    "#   w7 = getattr(rnn, f'{param}_l1')\n",
    "#   w8 = getattr(rnn, f'{param}_l1_reverse')\n",
    "  \n",
    "  \n",
    "#   print(((w1-w5)**2).mean(), ((w2-w6)**2).mean(), ((w3-w7)**2).mean(), ((w4-w8)**2).mean()) # should be zero\n",
    "#   print(((w1-w2)**2).mean(), ((w1-w6)**2).mean())\n",
    "\n",
    "torch.manual_seed(3407)\n",
    "ip = torch.randn((5,input_size))\n",
    "rnn_op,_ = rnn(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2998e-15, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Single Layer Unidirectional RNN using RNNCell\n",
    "h = []\n",
    "h.append(torch.zeros((hidden_size))) ## initial hidden state\n",
    "for t in range(len(ip)):\n",
    "  h.append(rnncell[0](ip[t], h[t]))\n",
    "h = torch.stack(h[1:])\n",
    "\n",
    "## Compare outputs. Should be 0 (or something like e^{-15} type super small value ~ effectively zero)\n",
    "print(((rnn_op - h)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1135e-16, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## MultiLayer Unidirectional RNN using RNNCell. (Left to Right Unfolding)\n",
    "h = []\n",
    "h_prev = [torch.zeros((hidden_size))]*num_layers ## initial hidden state\n",
    "for t in range(len(ip)):\n",
    "  h_t = []\n",
    "  for layer in range(num_layers):\n",
    "    if layer == 0:\n",
    "      h_t.append(rnncell[layer](ip[t], h_prev[layer]))\n",
    "    else:\n",
    "      h_t.append(rnncell[layer](h_t[layer-1], h_prev[layer]))\n",
    "  h.append(h_t[num_layers-1])\n",
    "  h_prev = h_t\n",
    "\n",
    "h = torch.stack(h)\n",
    "print(((rnn_op-h)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3092e-15, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Single Layer Bi-Directional RNN using RNNCell\n",
    "h, h_rev = [],[]\n",
    "h.append(torch.zeros((hidden_size))); h_rev.append(torch.zeros((hidden_size)))\n",
    "for t in range(len(ip)):\n",
    "  h.append(rnncell[0](ip[t],h[t]))\n",
    "  h_rev.append(rnncell_rev[0](ip[len(ip)-1-t],h_rev[t]))\n",
    "h = h[1:]; h_rev = h_rev[1:]\n",
    "h = torch.stack([torch.cat([x,y]) for x,y in zip(h,reversed(h_rev))])\n",
    "print(((rnn_op-h)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.4273e-16, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Multi Layer Bi-Directional RNN using RNNCell (Down to Up unfolding)\n",
    "h = []\n",
    "# h_prev = [torch.zeros((32))]*num_layers ## initial hidden state\n",
    "for layer in range(num_layers):\n",
    "  h_l = []; h_l.append(torch.zeros((hidden_size)))\n",
    "  h_l_rev = []; h_l_rev.append(torch.zeros((hidden_size)))\n",
    "  for t in range(len(ip)):\n",
    "    if layer == 0:\n",
    "      h_l.append(rnncell[layer](ip[t], h_l[t]))\n",
    "      h_l_rev.append(rnncell_rev[layer](ip[len(ip)-1-t], h_l_rev[t]))\n",
    "    else:\n",
    "      h_l_input = torch.cat([h_below[t], h_below_rev[len(ip)-1-t]]) # torch.cat([h_below_rev[len(ip)-1-t], h_below[t]])\n",
    "      h_l_rev_input = torch.cat([h_below[len(ip)-1-t], h_below_rev[t]]) # torch.cat([h_below_rev[t], h_below[len(ip)-1-t]])\n",
    "      h_l.append(rnncell[layer](h_l_input, h_l[t]))\n",
    "      h_l_rev.append(rnncell_rev[layer](h_l_rev_input, h_l_rev[t]))\n",
    "    \n",
    "  h_below = h_l[1:] ## below = one layer below; prev = one time step previous\n",
    "  h_below_rev = h_l_rev[1:]\n",
    "\n",
    "h_l = h_l[1:]; h_l_rev = h_l_rev[1:]\n",
    "h = torch.stack([torch.cat([x,y]) for x,y in zip(h_l,reversed(h_l_rev))])\n",
    "\n",
    "print(((rnn_op-h)**2).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('hagrid')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a31ace264ff6bdb66955af0418c53b88463898611b977e85836f12788415ae0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
